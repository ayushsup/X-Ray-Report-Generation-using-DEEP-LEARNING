{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2f1bf6d-bb5b-4a79-8301-e510a1a51b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, EncoderDecoderCache\n",
    "from tqdm import tqdm  # Import progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ed514d6-7e80-4af5-94d2-7d9146160ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\bagga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3dc9020-d598-4d68-926c-0af6a108fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = (224, 224)\n",
    "MODEL_NAME = \"t5-small\"  # Change to a medical-specific model if available\n",
    "MODEL_PATH = \"xray_report_model.pth\"\n",
    "data_path = \"processed_xray_data.csv\"\n",
    "image_folder = \"images/images_normalized\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bd7aef6-d857-4a6f-9108-801d7aa53816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5399fd0-d38d-4f1a-991f-e9841e84339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, dataframe, img_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_path = os.path.join(self.img_dir, row['filename'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        report_text = row['findings'] + \" \" + row['impression']\n",
    "        tokenized_text = self.tokenizer(report_text, padding='max_length', truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "        \n",
    "        return image, tokenized_text.input_ids.squeeze(), tokenized_text.attention_mask.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6c998d-371f-4cf8-9f79-65ebd3271501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class XRayReportGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XRayReportGenerator, self).__init__()\n",
    "        self.cnn = models.resnet50(pretrained=True)\n",
    "        self.cnn.fc = nn.Linear(2048, 512)  # Extract features\n",
    "        self.transformer = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    def forward(self, images, input_ids=None, attention_mask=None):\n",
    "        img_features = self.cnn(images)\n",
    "        encoder_outputs = self.transformer.get_encoder()(inputs_embeds=img_features.unsqueeze(1))\n",
    "        \n",
    "        if input_ids is not None:\n",
    "            decoder_input_ids = input_ids[:, :-1]  # Remove last token for teacher forcing\n",
    "            labels = input_ids[:, 1:].contiguous()  # Shifted target for loss computation\n",
    "            outputs = self.transformer(\n",
    "                input_ids=decoder_input_ids,\n",
    "                attention_mask=attention_mask[:, :-1],\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                labels=labels\n",
    "            )\n",
    "            return outputs.loss, outputs.logits\n",
    "        \n",
    "        # Inference Mode\n",
    "        generated_ids = self.transformer.generate(encoder_outputs=encoder_outputs, max_length=256)\n",
    "        return generated_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b58167-9859-47d6-a92e-8049689d0264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader\n",
    "dataset = XRayDataset(df, image_folder, transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9826a49-8e17-4c03-9afc-0cd1676a73f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = XRayReportGenerator().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb51ebdb-da4e-4c02-8d28-e41d1d46e389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trained model...\n",
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(\"Training model...\")\n",
    "    EPOCHS = 5\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")  # Add progress bar\n",
    "        for images, input_ids, attention_mask in progress_bar:\n",
    "            images, input_ids, attention_mask = images.to(DEVICE), input_ids.to(DEVICE), attention_mask.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            loss, logits = model(images, input_ids, attention_mask)  # Extract logits correctly\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())  # Live loss update\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}, Avg Loss: {avg_loss:.4f}\", flush=True)  # Ensure immediate print\n",
    "    \n",
    "    torch.save(model.state_dict(), MODEL_PATH)\n",
    "    print(\"Model saved successfully!\")\n",
    "else:\n",
    "    print(\"Loading trained model...\")\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ea4c09-6344-4b6a-91ad-50002013f003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Inference\n",
    "def generate_report(image_path):\n",
    "    if not os.path.exists(image_path):\n",
    "        print(\"Error: Image file not found!\", flush=True)\n",
    "        return\n",
    "    \n",
    "    model.eval()\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.forward(image)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    report = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    print(\"Generated Report:\", report, flush=True)\n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "149883cf-7be4-4a03-a64a-135e1eb1c83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Report: heart size and mediastinal contours are within normal limits. The lungs are clear. No acute disease.\n"
     ]
    }
   ],
   "source": [
    "# Example Usage\n",
    "sample_image_path = \"images/images_normalized/1000_IM-0003-2001.dcm.png\" # Change to actual image path\n",
    "if os.path.exists(sample_image_path):\n",
    "    generate_report(sample_image_path)\n",
    "else:\n",
    "    print(\"Error: Sample image not found. Please provide a valid image path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "978f563e-32f0-47a0-99b9-b3cd8e829c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1000_IM-0003-1001.dcm.png', '1000_IM-0003-2001.dcm.png', '1000_IM-0003-3001.dcm.png', '1001_IM-0004-1001.dcm.png', '1001_IM-0004-1002.dcm.png', '1002_IM-0004-1001.dcm.png', '1002_IM-0004-2001.dcm.png', '1003_IM-0005-2002.dcm.png', '1004_IM-0005-1001.dcm.png', '1004_IM-0005-2001.dcm.png']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"images/images_normalized\")[:10])  # Show first 10 images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f1d1b0b-8019-4008-9d67-1d2ad1944726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bagga\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\bagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\bagga\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Evaluating:   1%|▋                                                                     | 1/100 [00:00<00:44,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 1:\n",
      "Generated: heart size and mediastinal contours are within normal limits. The lungs are clear. No acute disease.\n",
      "Reference: The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax. Normal chest x-XXXX.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|█▍                                                                    | 2/100 [00:00<00:41,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 2:\n",
      "Generated: heart size and mediastinal contours are within normal limits. The lungs are clear. No acute disease.\n",
      "Reference: The cardiac silhouette and mediastinum size are within normal limits. There is no pulmonary edema. There is no focal consolidation. There are no XXXX of a pleural effusion. There is no evidence of pneumothorax. Normal chest x-XXXX.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   3%|██                                                                    | 3/100 [00:01<00:41,  2.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 3:\n",
      "Generated: heart is normal in size. The mediastinum is unremarkable. The lungs are clear. No acute disease.\n",
      "Reference: Borderline cardiomegaly. Midline sternotomy XXXX. Enlarged pulmonary arteries. Clear lungs. Inferior XXXX XXXX XXXX. No acute pulmonary findings.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████| 100/100 [00:40<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU score for the subset: 0.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure nltk tokenizer is available.\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text using NLTK's word_tokenize (convert to lowercase for consistency).\"\"\"\n",
    "    return nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Load your test CSV file (update the path if necessary).\n",
    "test_csv = \"processed_xray_data.csv\"  # Update if needed.\n",
    "test_df = pd.read_csv(test_csv)\n",
    "\n",
    "# Create the full test dataset using your previously defined XRayDataset.\n",
    "test_dataset = XRayDataset(test_df, image_folder, transform=transform)\n",
    "\n",
    "# Use a subset of the test data for faster evaluation (e.g., first 100 samples).\n",
    "subset_size = 100\n",
    "subset_indices = list(range(subset_size))\n",
    "test_dataset_subset = Subset(test_dataset, subset_indices)\n",
    "test_loader_subset = DataLoader(test_dataset_subset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Load your trained model.\n",
    "model = XRayReportGenerator()\n",
    "model.load_state_dict(torch.load(\"xray_report_model.pth\", map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Load the tokenizer separately.\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Lists to store tokenized references and hypotheses.\n",
    "all_references = []\n",
    "all_hypotheses = []\n",
    "\n",
    "# Evaluate the model on the test subset with progress tracking.\n",
    "with torch.no_grad():\n",
    "    for idx, (image, input_ids, attention_mask) in tqdm(enumerate(test_loader_subset),\n",
    "                                                          total=len(test_loader_subset),\n",
    "                                                          desc=\"Evaluating\"):\n",
    "        image = image.to(DEVICE)\n",
    "\n",
    "        # Generate predicted report tokens.\n",
    "        generated_ids = model(image)\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "        # Extract the reference text for the current sample.\n",
    "        # Since we're using a subset, test_df's row order corresponds to the subset indices.\n",
    "        reference_text = test_df.iloc[idx]['findings'] + \" \" + test_df.iloc[idx]['impression']\n",
    "\n",
    "        # Tokenize both generated and reference texts.\n",
    "        hyp_tokens = tokenize_text(generated_text)\n",
    "        ref_tokens = tokenize_text(reference_text)\n",
    "\n",
    "        all_hypotheses.append(hyp_tokens)\n",
    "        all_references.append([ref_tokens])  # Wrap reference tokens in a list to support multiple references.\n",
    "\n",
    "        # Optional: print the first three sample outputs for verification.\n",
    "        if idx < 3:\n",
    "            print(f\"\\nSample {idx+1}:\")\n",
    "            print(\"Generated:\", generated_text)\n",
    "            print(\"Reference:\", reference_text)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "# Compute the BLEU score using NLTK's corpus_bleu.\n",
    "bleu_score = corpus_bleu(all_references, all_hypotheses)\n",
    "print(f\"\\nBLEU score for the subset: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6d86b7b-ee0e-4f43-a22a-58b3d70254ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BLEU-1: 0.1460\n",
      "BLEU-2: 0.1064\n",
      "BLEU-3: 0.0797\n",
      "BLEU-4: 0.0607\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "smoothie = SmoothingFunction().method4  # Helps with short sentences or no matches\n",
    "\n",
    "# BLEU-1: weights = (1.0, 0, 0, 0)\n",
    "bleu1 = corpus_bleu(all_references, all_hypotheses, weights=(1.0, 0, 0, 0), smoothing_function=smoothie)\n",
    "\n",
    "# BLEU-2: weights = (0.5, 0.5, 0, 0)\n",
    "bleu2 = corpus_bleu(all_references, all_hypotheses, weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "\n",
    "# BLEU-3: weights = (0.33, 0.33, 0.33, 0)\n",
    "bleu3 = corpus_bleu(all_references, all_hypotheses, weights=(1/3, 1/3, 1/3, 0), smoothing_function=smoothie)\n",
    "\n",
    "# BLEU-4: weights = (0.25, 0.25, 0.25, 0.25)\n",
    "bleu4 = corpus_bleu(all_references, all_hypotheses, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nBLEU-1: {bleu1:.4f}\")\n",
    "print(f\"BLEU-2: {bleu2:.4f}\")\n",
    "print(f\"BLEU-3: {bleu3:.4f}\")\n",
    "print(f\"BLEU-4: {bleu4:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded36e48-a7ee-44d5-8f38-76a55a1f0d71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
